{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-27T05:04:07.072010Z",
     "iopub.status.busy": "2020-08-27T05:04:07.071793Z",
     "iopub.status.idle": "2020-08-27T05:04:07.127752Z",
     "shell.execute_reply": "2020-08-27T05:04:07.127297Z",
     "shell.execute_reply.started": "2020-08-27T05:04:07.071985Z"
    }
   },
   "source": [
    "## ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Intel: A Step in Reclaiming the PC Build Summit\n",
    "David D Lee | DSIR-720 | 8-28-2020\n",
    "\n",
    "## AMD and Build a PC Subreddit Predictor: Project Overview\n",
    "\n",
    "- Created Logistic Regression, Naive Bayes, and Long Short-term Memory Recurrent Neural Network models that predicts the subreddit of a post based off of the words in the title, and the content of the post.\n",
    "- Web Scraped both reddits pulling in over 100,000 posts each using Pushshift's API\n",
    "- Cleaned and vectorized the word data with 25,000 unigram features and roughly 160,000 bigram title and content features using sklearn.\n",
    "- Identified the words that are best associated with determining whether a post if for the AMD, of Build a Pc Subreddit.\n",
    "- Created an LSTM RNN model that can predict the Subreddit between AMD and BuildaPC using only the title with an 86.5% accuracy.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "#### How can we predict and market the computer parts consumers will buy, better than our competitors? (For Intel)\n",
    " \n",
    " \n",
    "\n",
    "### Contents:\n",
    "``` \n",
    "project_3-masterDL\n",
    "|__ Data \n",
    "|   |__ amd_big.csv  \n",
    "|   |__ build_pc_big.csv  \n",
    "|   |__ tf_df.csv  \n",
    "|__ images  \n",
    "|   |__ AMD_test.png  \n",
    "|   |__ build_pc_test.png  \n",
    "|   |__ common_words_amd.png  \n",
    "|   |__ common_words_build_pc.png  \n",
    "|   |__ LSTM_Score.png   \n",
    "|   |__ model1_title_logistic_regression.png  \n",
    "|   |__ model2_title__and_contents_logistic_regression.png  \n",
    "|   |__ model3a_title__and_contents_naive_bayes_gridsearch.png  \n",
    "|   |__ model3b_title__and_contents_logistic_regression_gridsearch.png  \n",
    "|   |__ rnn_loss.png  \n",
    "|   |__ rnn_accuracy.png  \n",
    "|   |__ rnn_loss2.png  \n",
    "|   |__ rnn_accuracy2.png  \n",
    "|__ 01_Web_Scaping_DL.ipynb (Web_Scraping)  \n",
    "|__ 02_Model_Benchmark.ipynb (Benchmarks)  \n",
    "|__ 03_Data_Cleaning_EDA_Models1&2.ipynb (EDA_and_model1&2)  \n",
    "|__ 04_Models_3a_and_3b_Logistic_Regression_and_Multinomial_NB_Models.ipynb (model3a_and_3b)  \n",
    "|__ 05_Model_4_LSTM_RNN_and_Conclusions.ipynb (lstm_nueral_net)  \n",
    "|__ Intel_presentation.png\n",
    "|__ README.md   \n",
    "```\n",
    "\n",
    "### Code and API\n",
    "Python Version: 3.7\n",
    "- [Packages: numpy, pandas, matplotlib, sklearn, regex, nltk, cufflinks, requests, time, seaborn, tensorflow, keras]\n",
    "- [Datasets scraped using Pushshift's API(https://github.com/pushshift/api)]\n",
    "\n",
    "\n",
    "## Web Scraping\n",
    "Using pushshift's API, a function was created that would loop through each of the chosen subreddits scraping 25 posts with Title and Content each.  A count of 5,000 was set, which looped until it completed, or an error was returned.\n",
    "\n",
    "![](./Images/web_scraping_details.png)\n",
    "\n",
    "## Data Cleaning\n",
    "The scraped data had several numbers and special characters that would interfere with our modeling.  Using regex, and by reading a number of posts, a function was created in removing irrelevant information.  In the content of a post(selftext), there were several posts that were removed by reddit or the user which showed as this: [removed].  These were also removed.\n",
    "\n",
    "\n",
    "## EDA\n",
    "By parsing out the the most common words, we were able to see that there were specific words that are centered around each subreddit, finding words that were common and unique to each category. \n",
    "\n",
    "There is no surprise that the word Ryzen followed by AMD are nearly 4 times more common than other words in determining a post.  \n",
    "Note that build and gaming are not unique to AMD’s reddit implying that pc builders and the gaming market are associated with both categories.\n",
    "\n",
    "We also noticed how Intel, MSI, and versus are on the unique words list implying that consumers are comparing vendors helping or hurting our market sentiment.\n",
    "![](./Images/common_words_amd.png)\n",
    "\n",
    "On our build a pc list, the words Pc and build are at the top of our list. Help is the 3rd most common word and it was 7th on the AMD list in the previous page.  Gaming is 6th most common word here and 8th for AMD.  \n",
    "\n",
    "It seems like AMD’s subreddit has similarities on words associated with the build a pc community, and perhaps focus their marketing towards them.  \n",
    "\n",
    "Looking at our unique words list , we see Advice,  budget, question, and time which also may be helpful in determining the market in the pc building community.\n",
    "\n",
    "It is worth investigating to see how these words change in popularity over time. It's a strong basis for building a classification model around.\n",
    "\n",
    "![](./Images/common_words_build_pc.png)\n",
    "\n",
    "## Benchmark:\n",
    "Before working on an optimal model, a baseline of 50% was established as the number of each subreddit are equal.\n",
    "\n",
    "A baseline model was also created using Multinomial Naive Bayes on the Title and Contents of a Reddit post before cleaning of the data.  \n",
    "\n",
    "The training set had an accuracy of 78.55%.\n",
    "The training set had an accuracy of 78.25%.\n",
    "\n",
    "These are the scores to beat.\n",
    "\n",
    "\n",
    "## Model Building\n",
    "Given the nature and complexity of language, the best approach for our modeling was to test multiple methods of Natural Language Processing to find an optimal solution.\n",
    "\n",
    "\n",
    "The models used in this project were Logistic Regression, Multinomial Naive Bayes, a Grid Search on each, along with a LSTM RNN model.\n",
    "\n",
    "\n",
    "![](./Images/model1_title_logistic_regression.png)  \n",
    "Model 1: Logistic Regression Model for Title only Confusion Matrix   \n",
    "ngram_range = (1,3)   \n",
    "max features 25000     \n",
    "min_df = 2   \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](./Images/model2_title__and_contents_logistic_regression.png)  \n",
    "Model 2: Logistic Regression Model with Title and Selftext Confusion Matrix   \n",
    "ngram_range = (1,2)   \n",
    "max features 25000     \n",
    "min_df = 2   \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "![](./Images/model3a_title__and_contents_naive_bayes_gridsearch.png)  \n",
    "Model 3a: Naive Bayes with Gridsearch with Title and Selftext Confusion Matrix    \n",
    "Best Parameters:  \n",
    "                {'cvec__max_features': 30000,  \n",
    "                'cvec__ngram_range': (1, 1),  \n",
    "                'cvec__stop_words': 'english'}  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](./Images/model3b_title__and_contents_logistic_regression_gridsearch.png)  \n",
    "Model 3b: Logistic Regression with Gridsearch with Title and Selftext Confusion Matrix      \n",
    "Best Parameters:  \n",
    "                {'cvec__max_features': 30000,  \n",
    "                 'cvec__ngram_range': (1, 2),  \n",
    "                 'cvec__stop_words': None}  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](./Images/rnn_accuracy.png)  \n",
    "![](./Images/rnn_loss.png)    \n",
    "![](./Images/LSTM_Score.png)    \n",
    "Model 4: Long Short-Term Memory Recurrent Neural Network on Title only       \n",
    "Best Parameters:    \n",
    "MAX WORDS = 50000  \n",
    "MAX SEQUENCE LENGTH = 500  \n",
    "EMBEDDING DIMENSIONS = 100  \n",
    "Spatial Dropout = 0.2  \n",
    "LSTM Layers = 100  \n",
    "Dropout = 0.2  \n",
    "Recurrent Dropout = 0.2  \n",
    "Dense Layer = 2  \n",
    "Activation = Softmax  \n",
    "Loss = Categorical Cross Entropy  \n",
    "Optimizer = adam  \n",
    "Metrics = Accuracy  \n",
    "epochs = 10  \n",
    "batch_size = 64  \n",
    "\n",
    "## Performance Results:\n",
    "Our train and test data results by model are as follows:\n",
    "- Model 1: Logistic Regression Model on Title\n",
    "    train score: 0.8816839803171131\n",
    "    test score: 0.8528704209950793  \n",
    "\n",
    "- Model 2: Logistic Regression Model with Title and Selftext   \n",
    "    train score: 0.9332604337525059\n",
    "    test score: 0.8841443411700383  \n",
    "    \n",
    "- Model 3a: Naive Bayes with Gridsearch with Title and Selftext  \n",
    "    train score: 0.8162140817690299\n",
    "    test score: 0.8104793147439402 \n",
    "   \n",
    "- Model 3b: Logistic Regression with Gridsearch  \n",
    "    train score: 0.9417289350586234\n",
    "    test score: 0.8875341716785129 \n",
    " \n",
    "- Model 4: Long Short-Term Memory Recurrent Neural Network Model (LSTM RNN) \n",
    "    training accuracy: 0.9417289350586234\n",
    "    testing accuracy: 0.8875341716785129\n",
    "\n",
    "\n",
    "\n",
    "## Conclusions:\n",
    "\n",
    "An AMD Post\n",
    "- https://www.reddit.com/r/Amd/comments/ii2mlv/radeon_driver_2083_vs_2014_2082_in_7_games_rx_570/  \n",
    "![](./Images/AMD_test.png)  \n",
    "\n",
    "A Build a PC Post\n",
    "- https://www.reddit.com/r/buildapc/comments/ihuxwa/im_a_bit_overwhelmed_with_the_many_different/  \n",
    "![](./Images/build_pc_test.png)   \n",
    "After successfully testing our model on predicting the Subreddits on unseen Reddit Titles correctly, we can determine that creating marketing and development strategies based on market sentiment is possible. By improving on the model, the possibilities of scraping several posts on various platforms on all computer manufacturers may  greatly benefit Intel on staying ahead of the technological curve, and regain dominance in the PC building market.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Next Steps\n",
    "With more time, creating a model that can routinely scrape more data on several computer parts manufacturers is ideal.  Being able to see how our model performs with several outcomes may provide insights on improving on our model.\n",
    "\n",
    "At the present state, I would include the contents with the titles to determine the increase in accuracy compared to my best Logistic Regression model.  \n",
    "\n",
    "After rigorous training, we would test the improved version of the model on social networks such as Facebook to see how well it can capture relevant data where a title tag is not present.\n",
    "\n",
    "## Additional Resources:\n",
    "Get Submission \n",
    "- https://pythonprogramming.altervista.org/collect-data-from-reddit/?doing_wp_cron=1597670992.0452320575714111328125\n",
    "\n",
    "Loop Inspiration: \n",
    "- https://www.textjuicer.com/2019/07/crawling-all-submissions-from-a-subreddit/\n",
    "\n",
    "Countdown for each iteration: \n",
    "- https://datatofish.com/while-loop-python/\n",
    "\n",
    "Ignore error:\n",
    "- https://stackoverflow.com/questions/38707513/ignoring-an-error-message-to-continue-with-the-loop-in-python    \n",
    "\n",
    "Interpreting Coefficients:\n",
    "- https://towardsdatascience.com/interpreting-coefficients-in-linear-and-logistic-regression-6ddf1295f6f1\n",
    "\n",
    "Optimize memory:\n",
    "- https://medium.com/@aakashgoel12/avoid-memory-error-techniques-to-reduce-dataframe-memory-usage-fcf53b2318a2\n",
    "\n",
    "Regex Cleaning:\n",
    "- https://stackoverflow.com/questions/30315035/strip-numbers-from-string-in-python\n",
    "\n",
    "Keras Embedding:\n",
    "- https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "- https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "\n",
    "Keras Sequential Model:\n",
    "- https://keras.io/guides/sequential_model/\n",
    "\n",
    "Drop out:\n",
    "- https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\n",
    "- https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/#:~:text=Long%20Short%2DTerm%20Memory%20\n",
    "\n",
    "Softmax:\n",
    "- https://medium.com/analytics-vidhya/softmax-classifier-using-tensorflow-on-mnist-dataset-with-sample-code-6538d0783b84\n",
    "- https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d\n",
    "\n",
    "LSTM:\n",
    "- https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046\n",
    "- https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17 \n",
    "- https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21#:~:text=An%20LSTM%20has%20a%20similar,operations%20within%20the%20LSTM's%20cells.&text=These%20operations%20are%20used%20to,to%20keep%20or%20forget%20information. \n",
    "\n",
    "Accuracy / Loss Plots: \n",
    "- https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Special thanks to Aiden Curley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
